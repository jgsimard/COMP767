{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP-767: Reinforcement Learning - Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prediction and control in RL [50 points] \n",
    "\n",
    "In this task, you will compare the performance of SARSA, expected SARSA and Q-learningon the Taxi domain from the Gym environment suite:https://gym.openai.com/envs/Taxi-v2/ \n",
    " \n",
    " Use a tabular representation of the state space, and ensure that the starting and end location of the passenger are random. Exploration should be softmax (Boltzmann).  You will need to run the following protocol.  You will do 10 independent runs.  Each run consists of 100 segments, in each segment there are 10 episodes of training, followed by 1 episode in which you simply run the optimal policy so far (i.e. you pick actions greedily based on the current value estimates). Pick 3 settings of the temperature parameter used in the exploration and 3 settings of the learning rate. \n",
    " \n",
    " You need to plot:\n",
    " \n",
    "* One u-shaped graph that shows the effect of the parameters on the final training performance, expressed as the return of the agent (averaged over the last 10 training episodes and the 10 runs); note that this will typically end up as an upside-down u.\n",
    " \n",
    "* One u-shaped graph that shows the effect of the parameters on the final testing performance, expressed as the return of the agent (during the final testing episode, averaged over the 10 runs)\n",
    " \n",
    "* Learning curves (mean and standard deviation computed based on the 10 runs) for what you pick as the best parameter setting for each algorithm\n",
    "\n",
    "Write a small report that describes your experiment,  your choices of parameters,  and the conclusions you draw from the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import temporal_difference\n",
    "import function_approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q(env):\n",
    "    num_actions = 6\n",
    "    num_states = 5*5*5*4\n",
    "    q = {state: np.random.rand(num_actions) for state in range(num_states)}\n",
    "\n",
    "    for loc_id in range(len(env.locs)):\n",
    "        loc = env.locs[loc_id]\n",
    "        s = env.encode(loc[0], loc[1], loc_id, loc_id)\n",
    "        q[s] = np.zeros(num_actions)\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_optimal_policy(q, env, discount_rate=0.9, show=True):\n",
    "    policy_greedy = temporal_difference.Greedy(n=6)\n",
    "    s = env.reset()\n",
    "    a = policy_greedy.get_action(q, s)\n",
    "    history = temporal_difference.History(s, a, discount_rate)\n",
    "    done = False\n",
    "    \n",
    "    if show : \n",
    "        env.render()\n",
    "        \n",
    "    while not done:\n",
    "        observation, reward, done, info = env.step(a)\n",
    "        s = observation\n",
    "        a = policy_greedy.get_action(q, s)\n",
    "        history.register(s, a, reward)\n",
    "        if show : \n",
    "            env.render()\n",
    "            print(f\"Timestep: {history.t}, Reward: {reward}, Total reward: {history.undiscounted_return()}, Discounted reward: {history.discounted_return()}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment :  0, Timestep : 200, Total reward : -2000\n",
      "Segment :  1, Timestep : 200, Total reward :  -200\n",
      "Segment :  2, Timestep : 200, Total reward :  -200\n",
      "Segment :  3, Timestep : 200, Total reward :  -200\n",
      "Segment :  4, Timestep : 200, Total reward :  -200\n",
      "Segment :  5, Timestep : 200, Total reward :  -200\n",
      "Segment :  6, Timestep : 200, Total reward :  -200\n",
      "Segment :  7, Timestep : 200, Total reward :  -200\n",
      "Segment :  8, Timestep : 200, Total reward :  -200\n",
      "Segment :  9, Timestep : 200, Total reward :  -200\n",
      "Segment : 10, Timestep : 200, Total reward :  -200\n",
      "Segment : 11, Timestep : 200, Total reward :  -200\n",
      "Segment : 12, Timestep : 200, Total reward :  -200\n",
      "Segment : 13, Timestep : 200, Total reward :  -200\n",
      "Segment : 14, Timestep : 200, Total reward :  -200\n",
      "Segment : 15, Timestep :   9, Total reward :    12\n",
      "Segment : 16, Timestep : 200, Total reward :  -200\n",
      "Segment : 17, Timestep :  15, Total reward :     6\n",
      "Segment : 18, Timestep : 200, Total reward :  -200\n",
      "Segment : 19, Timestep : 200, Total reward :  -200\n",
      "Segment : 20, Timestep : 200, Total reward :  -200\n",
      "Segment : 21, Timestep : 200, Total reward :  -200\n",
      "Segment : 22, Timestep : 200, Total reward :  -200\n",
      "Segment : 23, Timestep :  10, Total reward :    11\n",
      "Segment : 24, Timestep :   8, Total reward :    13\n",
      "Segment : 25, Timestep : 200, Total reward :  -200\n",
      "Segment : 26, Timestep :  12, Total reward :     9\n",
      "Segment : 27, Timestep :  12, Total reward :     9\n",
      "Segment : 28, Timestep :   9, Total reward :    12\n",
      "Segment : 29, Timestep : 200, Total reward :  -200\n",
      "Segment : 30, Timestep : 200, Total reward :  -200\n",
      "Segment : 31, Timestep : 200, Total reward :  -200\n",
      "Segment : 32, Timestep :  10, Total reward :    11\n",
      "Segment : 33, Timestep : 200, Total reward :  -200\n",
      "Segment : 34, Timestep :   9, Total reward :    12\n",
      "Segment : 35, Timestep : 200, Total reward :  -200\n",
      "Segment : 36, Timestep :  11, Total reward :    10\n",
      "Segment : 37, Timestep :   6, Total reward :    15\n",
      "Segment : 38, Timestep :  17, Total reward :     4\n",
      "Segment : 39, Timestep : 200, Total reward :  -200\n",
      "Segment : 40, Timestep :  14, Total reward :     7\n",
      "Segment : 41, Timestep :  13, Total reward :     8\n",
      "Segment : 42, Timestep :  16, Total reward :     5\n",
      "Segment : 43, Timestep : 200, Total reward :  -200\n",
      "Segment : 44, Timestep :  12, Total reward :     9\n",
      "Segment : 45, Timestep :  13, Total reward :     8\n",
      "Segment : 46, Timestep :  15, Total reward :     6\n",
      "Segment : 47, Timestep :   8, Total reward :    13\n",
      "Segment : 48, Timestep :  14, Total reward :     7\n",
      "Segment : 49, Timestep :  13, Total reward :     8\n",
      "Segment : 50, Timestep :  13, Total reward :     8\n",
      "Segment : 51, Timestep : 200, Total reward :  -200\n",
      "Segment : 52, Timestep : 200, Total reward :  -200\n",
      "Segment : 53, Timestep : 200, Total reward :  -200\n",
      "Segment : 54, Timestep :   9, Total reward :    12\n",
      "Segment : 55, Timestep :  13, Total reward :     8\n",
      "Segment : 56, Timestep :  15, Total reward :     6\n",
      "Segment : 57, Timestep :   9, Total reward :    12\n",
      "Segment : 58, Timestep : 200, Total reward :  -200\n",
      "Segment : 59, Timestep :  12, Total reward :     9\n",
      "Segment : 60, Timestep :  11, Total reward :    10\n",
      "Segment : 61, Timestep :  14, Total reward :     7\n",
      "Segment : 62, Timestep :  14, Total reward :     7\n",
      "Segment : 63, Timestep :  15, Total reward :     6\n",
      "Segment : 64, Timestep : 200, Total reward :  -200\n",
      "Segment : 65, Timestep :  13, Total reward :     8\n",
      "Segment : 66, Timestep :  14, Total reward :     7\n",
      "Segment : 67, Timestep : 200, Total reward :  -200\n",
      "Segment : 68, Timestep :  17, Total reward :     4\n",
      "Segment : 69, Timestep :  12, Total reward :     9\n",
      "Segment : 70, Timestep :   9, Total reward :    12\n",
      "Segment : 71, Timestep :  14, Total reward :     7\n",
      "Segment : 72, Timestep :  12, Total reward :     9\n",
      "Segment : 73, Timestep :   9, Total reward :    12\n",
      "Segment : 74, Timestep :  14, Total reward :     7\n",
      "Segment : 75, Timestep :  13, Total reward :     8\n",
      "Segment : 76, Timestep :  18, Total reward :     3\n",
      "Segment : 77, Timestep :  16, Total reward :     5\n",
      "Segment : 78, Timestep :  13, Total reward :     8\n",
      "Segment : 79, Timestep :  10, Total reward :    11\n",
      "Segment : 80, Timestep :  12, Total reward :     9\n",
      "Segment : 81, Timestep :  12, Total reward :     9\n",
      "Segment : 82, Timestep :  10, Total reward :    11\n",
      "Segment : 83, Timestep :  16, Total reward :     5\n",
      "Segment : 84, Timestep :  12, Total reward :     9\n",
      "Segment : 85, Timestep :  16, Total reward :     5\n",
      "Segment : 86, Timestep :  12, Total reward :     9\n",
      "Segment : 87, Timestep :   9, Total reward :    12\n",
      "Segment : 88, Timestep :   9, Total reward :    12\n",
      "Segment : 89, Timestep : 200, Total reward :  -200\n",
      "Segment : 90, Timestep :  16, Total reward :     5\n",
      "Segment : 91, Timestep :  10, Total reward :    11\n",
      "Segment : 92, Timestep :  11, Total reward :    10\n",
      "Segment : 93, Timestep :  13, Total reward :     8\n",
      "Segment : 94, Timestep :  11, Total reward :    10\n",
      "Segment : 95, Timestep :  14, Total reward :     7\n",
      "Segment : 96, Timestep :  13, Total reward :     8\n",
      "Segment : 97, Timestep :  13, Total reward :     8\n",
      "Segment : 98, Timestep :  14, Total reward :     7\n",
      "Segment : 99, Timestep :  13, Total reward :     8\n"
     ]
    }
   ],
   "source": [
    "def run(update_algorithm, \n",
    "        temperature_factor=1, \n",
    "        learning_rate=0.5, \n",
    "        n_segment=100, \n",
    "        episode_by_segment=10, \n",
    "        discount_rate = 0.9, \n",
    "        show=False, \n",
    "        print_segment=False):\n",
    "    \n",
    "    exploration_policy = temporal_difference.SoftmaxExploration(temperature_factor=temperature_factor)\n",
    "    env = gym.make('Taxi-v2')\n",
    "    q = get_q(env.unwrapped)\n",
    "    \n",
    "    training_results=[]\n",
    "    testing_results = []\n",
    "    for segment in range(n_segment):\n",
    "        for episode in range(episode_by_segment):\n",
    "            episode_history = update_algorithm(env, exploration_policy, q, learning_rate, discount_rate)\n",
    "            training_results.append(episode_history.undiscounted_return())\n",
    "        \n",
    "        testing_history = test_optimal_policy(q, env, discount_rate, show)\n",
    "        testing_results.append(testing_history.undiscounted_return())\n",
    "        \n",
    "        if print_segment:\n",
    "            print(\"Segment : %2i, Timestep : %3i, Total reward : %5i\" % (segment, \n",
    "                                                                         testing_history.t, \n",
    "                                                                         testing_history.undiscounted_return()))\n",
    "    return training_results, testing_results\n",
    "\n",
    "\n",
    "training_results, testing_results = run(temporal_difference.qlearning_update, print_segment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot(name, algo, results, learning_rates, temperature_factors):\n",
    "    for i in range(len(results)):\n",
    "        plt.plot(learning_rates, results[i,:], label=temperature_factors[i])\n",
    "    plt.legend()\n",
    "    plt.title(algo + \" during \" + name)\n",
    "    plt.xlabel(\"learning rate\")\n",
    "    plt.ylabel(\"Cumulative reward\")\n",
    "    \n",
    "def plot_parameter_effect(algo, train, test, learning_rates, temperature_factors):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    subplot(\"training\", algo, train, learning_rates, temperature_factors)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    subplot(\"testing\", algo, test, learning_rates, temperature_factors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def experiment(update_algorithm, temparature_factor, learning_rate):\n",
    "    n_run = 10\n",
    "    n_segment=100\n",
    "    episode_by_segment=10\n",
    "    \n",
    "    train = np.zeros((n_run, n_segment*episode_by_segment))\n",
    "    test = np.zeros((n_run,n_segment))\n",
    "    for i in range(n_run):\n",
    "        train[i,:], test[i,:] = run(update_algorithm, temparature_factor, learning_rate, n_segment, episode_by_segment)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def final_training_performance(train):\n",
    "    return np.mean(train[:,-10:])\n",
    "\n",
    "def final_testing_performance(test):\n",
    "    return np.mean(test[:,-1:])\n",
    "\n",
    "def filter_results(r):\n",
    "    train, test = [], []\n",
    "    for train_, test_ in r:\n",
    "        train.append(final_training_performance(train_))\n",
    "        test.append(final_testing_performance(test_))\n",
    "    \n",
    "    train = np.array(train).reshape(3,3)\n",
    "    test = np.array(test).reshape(3,3)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def get_best(test, learning_rates, temperature_factors):\n",
    "    argmax = test.argmax()\n",
    "    tf_index, lr_index = np.unravel_index(argmax, test.shape)\n",
    "    return test[tf_index, lr_index], learning_rates[lr_index], temperature_factors[tf_index], argmax\n",
    "\n",
    "def get_learning_curve(r, amax, train=True):\n",
    "    i = 0 if train else 1\n",
    "    return np.mean(r[amax][i], axis=0), np.std(r[amax][i], axis=0)\n",
    "\n",
    "def plot_learning_curve(r, amax):\n",
    "    mean, std = get_learning_curve(r, amax, train=True)\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(mean)\n",
    "    plt.title(\"Mean return during training\")\n",
    "    plt.xlabel(\"learning step\")\n",
    "    plt.ylabel(\"Mean\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(std)\n",
    "    plt.title(\"Standard deviation of the return during training\")\n",
    "    plt.xlabel(\"learning step\")\n",
    "    plt.ylabel(\"Standard deviation\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def algorithm_parameters_effect_parallel(update_algorithm, learning_rates, temperature_factors):\n",
    "    print(update_algorithm)\n",
    "    sys.stdout.flush()\n",
    "    p = Pool(10)\n",
    "\n",
    "    settings=[]\n",
    "    for temparature_factor in temperature_factors:\n",
    "        for learning_rate in learning_rates:\n",
    "            settings.append((algorithms[update_algorithm], temparature_factor, learning_rate))\n",
    "    r = p.starmap(experiment, settings)\n",
    "    \n",
    "    train, test = filter_results(r)\n",
    "\n",
    "\n",
    "    plot_parameter_effect(update_algorithm, train, test, learning_rates, temperature_factors)\n",
    "    max_return, lr, tf, amax = get_best(test, learning_rates, temperature_factors)\n",
    "    print(f\"Best return : {max_return}, learning rate = {lr}, and temperature factor = {tf} \\n\")\n",
    "    \n",
    "    plot_learning_curve(r, amax)\n",
    "\n",
    "algorithms = {\"SARSA\":temporal_difference.sarsa_update, \n",
    "              \"Expected SARSA\": temporal_difference.expected_sarsa_update, \n",
    "              \"Q-learning\":temporal_difference.qlearning_update}\n",
    "temperature_factors= [0.5, 1, 2]\n",
    "learning_rates = [0.1, 0.5, 1]\n",
    "\n",
    "# for algo in algorithms:\n",
    "#     algorithm_parameters_effect_parallel(algo, learning_rates, temperature_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE SOME TEXT to explain the graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Function approximation [50 points]\n",
    "Implement and compare empirically Monte Carlo and TD-learning with eligibility traces and linear function approximation on the Pendulum-v0 domain from the Gym environment suite:https://gym.openai.com/envs/Pendulum-v0/ \n",
    "\n",
    "You should evaluate the fixed policy that produces torque in the same direction as the current velocity with probability 0.9 and in the opposite direction with probability 0.1. If velocity is 0, you can torque in a random direction. \n",
    "\n",
    "For this experiment, you should use a tile coding function  approximator, in which you discretize  the  angular  position  and angular velocity into 10 bins each, and use 5 overlapping tilings, whose weights start initialized randomly between −0.001 and 0.001. \n",
    "\n",
    "You will need to use the same seed for this initialization for all parameters settings, but will have 10 different seeds (for the different runs). You should use values of λ={0, 0.3, 0.7, 0.9, 1}. For each value, use 3 settings of the learning rate parameter α = {1/4, 1/8, 1/16}. Remember that the learning rate per parameter needs to be divided by the number of overlapping tilings. \n",
    "\n",
    "Perform 10 independent runs, each of 200 episodes. Each episode should start art state (0,0).  Plot 5 graphs, one for each of the λ-values, showing the value of the start state, using each of the α-values (each of the 5 graphs has 3 curves). Explain briefly what you can conclude from these graphs, in terms of the speed of convergence and stability of these algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_parameters = [0, 0.3, 0.7, 0.9, 1]\n",
    "weights__max_initial_range = 0.001\n",
    "learning_rates = [1/4, 1/8, 1/16]\n",
    "n_run = 10\n",
    "n_episode_by_run = 200\n",
    "n_bins = 10\n",
    "n_tilings = 5\n",
    "probability_torque_same_direction = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(env, state):\n",
    "    velocity = state[1]\n",
    "    action = np.array([env.unwrapped.max_torque * np.sign(velocity)])\n",
    "    \n",
    "    p = np.random.rand(1)\n",
    "    if p > probability_torque_same_direction:\n",
    "        action *= -1\n",
    "    return action\n",
    "\n",
    "def get_angle_velocity(observation):\n",
    "    angle = np.arccos(observation[0])\n",
    "    if observation[1] < 0:\n",
    "        angle = 2 * np.pi - angle\n",
    "    velocity = observation[2]\n",
    "    return angle, velocity\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "state_observation_space = gym.spaces.Box(low=np.array([0.0, -env.unwrapped.max_speed]), \n",
    "                                         high=np.array([2*np.pi, env.unwrapped.max_speed]),\n",
    "                                         dtype=np.float32)\n",
    "approximation_function = function_approximation.TileCoding2D(n_bins=n_bins, \n",
    "                                                             n_tilings=n_tilings,\n",
    "                                                             bounds_box = state_observation_space)\n",
    "\n",
    "weights = np.random.rand(n_bins**2 * n_tilings) * weights__max_initial_range * 2 - weights__max_initial_range\n",
    "eligibity_trace = np.zeros_like(weights)\n",
    "\n",
    "for episode in range(n_episode_by_run):\n",
    "    temporal_difference.semi_gradient_td_update(env, policy, \n",
    "                                                learning_rate=0.5/n_tilings, \n",
    "                                                discount_rate=0.9,\n",
    "                                                lambda_return=0.1, \n",
    "                                                approximation_function=approximation_function, \n",
    "                                                weights = weights, \n",
    "                                                eligibity_trace=eligibity_trace, \n",
    "                                                state_from_observation_function= get_angle_velocity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
